import os
import json
import argparse
import random
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import numpy as np
from PIL import Image
from tqdm import tqdm


# æ ‡ç­¾ç¼–ç 
LABEL_MAP = {
    'normal': 0,
    'benign': 1,
    'malignant': 2,
}


def collect_busi_samples(busi_root: str) -> List[Dict]:
    """
    éå† BUSI æ ¹ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯
    
    Args:
        busi_root: BUSI æ•°æ®é›†æ ¹ç›®å½•
    
    Returns:
        æ ·æœ¬å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«:
            - id: å”¯ä¸€æ ‡è¯†ç¬¦
            - img_path: åŸå§‹å›¾åƒè·¯å¾„
            - mask_path: åŸå§‹ mask è·¯å¾„ï¼ˆæˆ– Noneï¼‰
            - label_name: ç±»åˆ«åç§° ('benign', 'malignant', 'normal')
    """
    busi_root = Path(busi_root)
    if not busi_root.exists():
        raise FileNotFoundError(f"BUSI root directory not found: {busi_root}")
    
    samples = []
    
    # å¤„ç†å„ä¸ªç±»åˆ«ç›®å½•
    for category in ['benign', 'malignant', 'normal']:
        category_dir = busi_root / category
        
        if not category_dir.exists():
            print(f"Warning: Category directory not found: {category_dir}")
            continue
        
        print(f"\nProcessing {category} category...")
        
        if category in ['benign', 'malignant']:
            # æœ‰ mask çš„ç±»åˆ«ï¼šéœ€è¦é…å¯¹ image å’Œ mask
            samples_in_category = _collect_paired_samples(category_dir, category)
        else:
            # normal ç±»åˆ«ï¼šåªæœ‰å›¾åƒï¼Œæ²¡æœ‰ mask
            samples_in_category = _collect_unpaired_samples(category_dir, category)
        
        samples.extend(samples_in_category)
        print(f"  Collected {len(samples_in_category)} {category} samples")
    
    print(f"\nTotal collected samples: {len(samples)}")
    return samples


def _collect_paired_samples(category_dir: Path, category: str) -> List[Dict]:
    """
    æ”¶é›†æœ‰ mask çš„æ ·æœ¬ï¼ˆbenign / malignantï¼‰
    
    Args:
        category_dir: ç±»åˆ«ç›®å½•è·¯å¾„
        category: ç±»åˆ«åç§°
    
    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    samples = []
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼ˆä¸åŒ…å« _maskï¼‰
    all_files = sorted(category_dir.glob('*.png'))
    image_files = [f for f in all_files if '_mask' not in f.name]
    
    sample_idx = 1
    for img_path in image_files:
        # æ„é€ å¯¹åº”çš„ mask æ–‡ä»¶è·¯å¾„
        # ä¾‹å¦‚: "benign (1).png" -> "benign (1)_mask.png"
        mask_name = img_path.stem + '_mask.png'
        mask_path = category_dir / mask_name
        
        if not mask_path.exists():
            print(f"  Warning: Mask not found for {img_path.name}, skipping...")
            continue
        
        # ç”Ÿæˆå”¯ä¸€ ID
        sample_id = f"BUSI_{category}_{sample_idx:06d}"
        
        samples.append({
            'id': sample_id,
            'img_path': str(img_path),
            'mask_path': str(mask_path),
            'label_name': category,
        })
        
        sample_idx += 1
    
    return samples


def _collect_unpaired_samples(category_dir: Path, category: str) -> List[Dict]:
    """
    æ”¶é›†æ—  mask çš„æ ·æœ¬ï¼ˆnormalï¼‰
    
    Args:
        category_dir: ç±»åˆ«ç›®å½•è·¯å¾„
        category: ç±»åˆ«åç§°
    
    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    samples = []
    
    # è·å–æ‰€æœ‰ PNG æ–‡ä»¶
    image_files = sorted(category_dir.glob('*.png'))
    
    for idx, img_path in enumerate(image_files, start=1):
        # ç”Ÿæˆå”¯ä¸€ ID
        sample_id = f"BUSI_{category}_{idx:06d}"
        
        samples.append({
            'id': sample_id,
            'img_path': str(img_path),
            'mask_path': None,
            'label_name': category,
        })
    
    return samples


def split_samples(
    samples: List[Dict],
    val_ratio: float = 0.2,
    test_ratio: float = 0.0,
    seed: int = 42,
) -> Tuple[List[Dict], List[Dict], Optional[List[Dict]]]:
    """
    å°†æ ·æœ¬åˆ’åˆ†ä¸º train / val / test
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        (train_samples, val_samples, test_samples)
        test_samples å¯èƒ½ä¸º Noneï¼ˆå½“ test_ratio=0ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # æ‰“ä¹±æ ·æœ¬
    samples_shuffled = samples.copy()
    random.shuffle(samples_shuffled)
    
    n_total = len(samples_shuffled)
    
    # è®¡ç®—å„ä¸ª split çš„æ ·æœ¬æ•°
    if test_ratio > 0:
        n_test = int(n_total * test_ratio)
        n_val = int((n_total - n_test) * val_ratio)
        n_train = n_total - n_test - n_val
        
        test_samples = samples_shuffled[:n_test]
        val_samples = samples_shuffled[n_test:n_test + n_val]
        train_samples = samples_shuffled[n_test + n_val:]
    else:
        n_val = int(n_total * val_ratio)
        n_train = n_total - n_val
        
        val_samples = samples_shuffled[:n_val]
        train_samples = samples_shuffled[n_val:]
        test_samples = None
    
    print(f"\nDataset split:")
    print(f"  Train: {len(train_samples)} samples")
    print(f"  Val: {len(val_samples)} samples")
    if test_samples is not None:
        print(f"  Test: {len(test_samples)} samples")
    
    return train_samples, val_samples, test_samples


def load_and_convert_image(img_path: str) -> np.ndarray:
    """
    åŠ è½½å¹¶è½¬æ¢å›¾åƒä¸ºç°åº¦ float32 æ•°ç»„
    
    Args:
        img_path: å›¾åƒè·¯å¾„
    
    Returns:
        np.ndarray, shape (H, W), dtype float32
    """
    img = Image.open(img_path).convert('L')  # è½¬ä¸ºç°åº¦
    img_np = np.array(img, dtype=np.float32)
    return img_np


def load_and_convert_mask(mask_path: str) -> np.ndarray:
    """
    åŠ è½½å¹¶è½¬æ¢ mask ä¸ºäºŒå€¼æ•°ç»„
    
    Args:
        mask_path: mask è·¯å¾„
    
    Returns:
        np.ndarray, shape (H, W), dtype float32, å€¼ä¸º 0 æˆ– 1
    """
    mask = Image.open(mask_path).convert('L')  # è½¬ä¸ºç°åº¦
    mask_np = np.array(mask, dtype=np.float32)
    
    # äºŒå€¼åŒ–: >0 ä¸º 1ï¼Œå¦åˆ™ä¸º 0
    mask_np = (mask_np > 0).astype(np.float32)
    
    return mask_np


def preprocess_and_save(
    samples: List[Dict],
    output_root: str,
    val_ratio: float = 0.2,
    test_ratio: float = 0.0,
    seed: int = 42,
) -> None:
    """
    é¢„å¤„ç†æ ·æœ¬å¹¶ä¿å­˜ä¸º npy + metadata JSON
    
    Args:
        samples: æ”¶é›†åˆ°çš„æ ·æœ¬åˆ—è¡¨
        output_root: è¾“å‡ºæ ¹ç›®å½•
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    """
    output_root = Path(output_root)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    images_dir = output_root / 'images'
    masks_dir = output_root / 'masks'
    images_dir.mkdir(parents=True, exist_ok=True)
    masks_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nOutput directory: {output_root}")
    print(f"  Images: {images_dir}")
    print(f"  Masks: {masks_dir}")
    
    # åˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_samples(
        samples, val_ratio, test_ratio, seed
    )
    
    # å¤„ç†æ‰€æœ‰æ ·æœ¬
    all_splits = {
        'train': train_samples,
        'val': val_samples,
    }
    if test_samples is not None:
        all_splits['test'] = test_samples
    
    metadata_all = {}
    
    for split_name, samples_in_split in all_splits.items():
        print(f"\nProcessing {split_name} split ({len(samples_in_split)} samples)...")
        
        metadata = []
        
        for sample in tqdm(samples_in_split, desc=f"  Converting {split_name}"):
            sample_id = sample['id']
            
            # åŠ è½½å¹¶ä¿å­˜å›¾åƒ
            img_np = load_and_convert_image(sample['img_path'])
            img_save_path = images_dir / f"{sample_id}.npy"
            np.save(img_save_path, img_np)
            
            # åŠ è½½å¹¶ä¿å­˜ maskï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if sample['mask_path'] is not None:
                # æœ‰ mask çš„æ ·æœ¬ï¼šåŠ è½½çœŸå® mask
                mask_np = load_and_convert_mask(sample['mask_path'])
            else:
                # æ²¡æœ‰ mask çš„æ ·æœ¬ï¼ˆnormalï¼‰ï¼šç”Ÿæˆå…¨é»‘ mask
                mask_np = np.zeros_like(img_np, dtype=np.float32)
            
            # ä¿å­˜ maskï¼ˆç°åœ¨æ‰€æœ‰æ ·æœ¬éƒ½æœ‰ maskï¼‰
            mask_save_path = masks_dir / f"{sample_id}_mask.npy"
            np.save(mask_save_path, mask_np)
            
            # æ„é€  metadata æ¡ç›®
            meta_entry = {
                'id': sample_id,
                'image_path': f"images/{sample_id}.npy",
                'mask_path': f"masks/{sample_id}_mask.npy" if mask_save_path else None,
                'label': LABEL_MAP[sample['label_name']],
                'domain': 'BUSI',
            }
            
            metadata.append(meta_entry)
        
        metadata_all[split_name] = metadata
    
    # ä¿å­˜ metadata JSON
    for split_name, metadata in metadata_all.items():
        json_path = output_root / f'{split_name}_meta.json'
        with open(json_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"\nSaved {split_name} metadata: {json_path}")
        print(f"  Samples: {len(metadata)}")
    
    print(f"\nâœ… Preprocessing complete!")


def quick_test(output_root: str, image_size: Tuple[int, int] = (256, 256)) -> None:
    """
    å¿«é€Ÿæµ‹è¯•ï¼šéªŒè¯é¢„å¤„ç†ç»“æœå¯ä»¥è¢« DataLoader æ­£ç¡®åŠ è½½
    
    Args:
        output_root: é¢„å¤„ç†è¾“å‡ºç›®å½•
        image_size: æµ‹è¯•æ—¶çš„å›¾åƒå°ºå¯¸
    """
    print("\n" + "=" * 60)
    print("Quick Test: DataLoader Compatibility")
    print("=" * 60)
    
    output_root = Path(output_root)
    train_meta_path = output_root / 'train_meta.json'
    
    if not train_meta_path.exists():
        print(f"Error: Metadata file not found: {train_meta_path}")
        return
    
    # å°è¯•å¯¼å…¥é€šç”¨ DataLoader
    try:
        # å‡è®¾ dataset æ¨¡å—åœ¨ Python path ä¸­
        import sys
        
        # å°è¯•æ·»åŠ å¯èƒ½çš„è·¯å¾„
        possible_paths = [
            Path.cwd() / 'dataset',
            Path.cwd().parent / 'dataset',
        ]
        for p in possible_paths:
            if p.exists():
                sys.path.insert(0, str(p.parent))
                break
        
        from dataset.meta.UltrasoundSample import load_ultrasound_metadata
        from dataset.dataset.ultrasound_segmentation import UltrasoundSegmentationDataset
        from torch.utils.data import DataLoader
        import torch
        
        print("\nâœ… Successfully imported DataLoader components")
        
        # åŠ è½½ metadata
        # éœ€è¦ä½¿ç”¨ç»å¯¹è·¯å¾„
        with open(train_meta_path, 'r') as f:
            metadata_raw = json.load(f)
        
        samples = load_ultrasound_metadata(str(train_meta_path))
        print(f"\nâœ… Loaded {len(samples)} samples from metadata")
        
        # åˆ›å»º Dataset
        dataset = UltrasoundSegmentationDataset(
            samples=samples,
            image_size=image_size,
            augment=None,
            normalize=True,
            return_label=True,
            return_domain=True,
        )
        
        # åˆ›å»º DataLoader
        loader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,
        )
        
        # è·å–ä¸€ä¸ª batch
        batch = next(iter(loader))
        
        print(f"\nâœ… Successfully loaded one batch:")
        print(f"  images: {batch['image'].shape}")
        if batch['mask'] is not None and batch['mask'][0] is not None:
            print(f"  masks: {batch['mask'].shape}")
        else:
            print(f"  masks: None (normal samples)")
        print(f"  labels: {batch['label']}")
        print(f"  domains: {batch['domain']}")
        
        # éªŒè¯æ•°æ®èŒƒå›´
        print(f"\nData statistics:")
        print(f"  Image range: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]")
        print(f"  Image mean: {batch['image'].mean():.3f}")
        
        if batch['mask'] is not None and batch['mask'][0] is not None:
            unique_mask_values = torch.unique(batch['mask'])
            print(f"  Mask unique values: {unique_mask_values.tolist()}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        # temp_meta_path.unlink()
        
        print("\nâœ… DataLoader compatibility test passed!")
        
    except ImportError as e:
        print(f"\nâš ï¸  Could not import DataLoader components: {e}")
        print("Skipping DataLoader test. Please ensure dataset modules are in Python path.")
        
        # ç®€å•éªŒè¯ï¼šç›´æ¥è¯»å–æ–‡ä»¶
        print("\nPerforming basic file validation instead...")
        
        with open(train_meta_path, 'r') as f:
            metadata = json.load(f)
        
        print(f"\nMetadata validation:")
        print(f"  Total entries: {len(metadata)}")
        
        # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬
        for i, item in enumerate(metadata[:3]):
            print(f"\nSample {i+1}:")
            print(f"  ID: {item['id']}")
            
            img_path = output_root / item['image_path']
            print(f"  Image exists: {img_path.exists()}")
            
            if item['mask_path']:
                mask_path = output_root / item['mask_path']
                print(f"  Mask exists: {mask_path.exists()}")
            else:
                print(f"  Mask: None (expected for normal samples)")
            
            print(f"  Label: {item['label']}")
            print(f"  Domain: {item['domain']}")
        
        print("\nâœ… Basic file validation passed!")
    
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(
        description="Preprocess BUSI dataset to unified format"
    )
    
    parser.add_argument(
        '--busi_root',
        type=str,
        required=True,
        help='Path to original BUSI dataset root directory'
    )
    
    parser.add_argument(
        '--output_root',
        type=str,
        required=True,
        help='Path to output directory for preprocessed data'
    )
    
    parser.add_argument(
        '--val_ratio',
        type=float,
        default=0.2,
        help='Validation set ratio (default: 0.2)'
    )
    
    parser.add_argument(
        '--test_ratio',
        type=float,
        default=0.0,
        help='Test set ratio (default: 0.0, no test set)'
    )
    
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for dataset splitting (default: 42)'
    )
    
    parser.add_argument(
        '--overwrite',
        action='store_true',
        help='Allow overwriting existing output directory'
    )
    
    parser.add_argument(
        '--skip_test',
        action='store_true',
        help='Skip the quick DataLoader test after preprocessing'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_root = Path(args.output_root)
    if output_root.exists() and not args.overwrite:
        response = input(
            f"Output directory already exists: {output_root}\n"
            f"Do you want to overwrite it? (y/n): "
        )
        if response.lower() != 'y':
            print("Aborted.")
            return
        else:
            shutil.rmtree(output_root)
    
    # å¼€å§‹é¢„å¤„ç†
    print("=" * 60)
    print("BUSI Dataset Preprocessing")
    print("=" * 60)
    print(f"\nInput: {args.busi_root}")
    print(f"Output: {args.output_root}")
    print(f"Val ratio: {args.val_ratio}")
    print(f"Test ratio: {args.test_ratio}")
    print(f"Random seed: {args.seed}")
    
    # æ”¶é›†æ ·æœ¬
    samples = collect_busi_samples(args.busi_root)
    
    if len(samples) == 0:
        print("\nError: No samples collected. Please check the input directory structure.")
        return
    
    # é¢„å¤„ç†å¹¶ä¿å­˜
    preprocess_and_save(
        samples=samples,
        output_root=args.output_root,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        seed=args.seed,
    )
    
    # å¿«é€Ÿæµ‹è¯•
    if not args.skip_test:
        quick_test(args.output_root)
    
    print("\n" + "=" * 60)
    print("All done! ğŸ‰")
    print("=" * 60)


if __name__ == '__main__':
    main()