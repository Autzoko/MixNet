import os
import json
import argparse
import random
import shutil
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import numpy as np
from PIL import Image
from tqdm import tqdm


# æ ‡ç­¾ç¼–ç  - ğŸ”¥ ç§»é™¤ normal
LABEL_MAP = {
    'benign': 1,
    'malignant': 2,
}


def collect_busi_samples(busi_root: str) -> List[Dict]:
    """
    éå† BUSI æ ¹ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯
    
    ğŸ”¥ ä¿®æ”¹ï¼šè·³è¿‡ normal ç±»å‹æ ·æœ¬ï¼Œåªæ”¶é›† benign å’Œ malignant
    
    Args:
        busi_root: BUSI æ•°æ®é›†æ ¹ç›®å½•
    
    Returns:
        æ ·æœ¬å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«:
            - id: å”¯ä¸€æ ‡è¯†ç¬¦
            - img_path: åŸå§‹å›¾åƒè·¯å¾„
            - mask_path: åŸå§‹ mask è·¯å¾„
            - label_name: ç±»åˆ«åç§° ('benign', 'malignant')
    """
    busi_root = Path(busi_root)
    if not busi_root.exists():
        raise FileNotFoundError(f"BUSI root directory not found: {busi_root}")
    
    samples = []
    
    # ğŸ”¥ åªå¤„ç† benign å’Œ malignantï¼Œè·³è¿‡ normal
    for category in ['benign', 'malignant']:
        category_dir = busi_root / category
        
        if not category_dir.exists():
            print(f"Warning: Category directory not found: {category_dir}")
            continue
        
        print(f"\nProcessing {category} category...")
        
        # æ”¶é›†é…å¯¹æ ·æœ¬ï¼ˆæœ‰ maskï¼‰
        samples_in_category = _collect_paired_samples(category_dir, category)
        
        samples.extend(samples_in_category)
        print(f"  Collected {len(samples_in_category)} {category} samples")
    
    print(f"\nTotal collected samples: {len(samples)}")
    print(f"ğŸ”¥ Normal samples excluded from dataset")
    return samples


def _collect_paired_samples(category_dir: Path, category: str) -> List[Dict]:
    """
    æ”¶é›†æœ‰ mask çš„æ ·æœ¬ï¼ˆbenign / malignantï¼‰
    
    Args:
        category_dir: ç±»åˆ«ç›®å½•è·¯å¾„
        category: ç±»åˆ«åç§°
    
    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    samples = []
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼ˆä¸åŒ…å« _maskï¼‰
    all_files = sorted(category_dir.glob('*.png'))
    image_files = [f for f in all_files if '_mask' not in f.name]
    
    sample_idx = 1
    for img_path in image_files:
        # æ„é€ å¯¹åº”çš„ mask æ–‡ä»¶è·¯å¾„
        # ä¾‹å¦‚: "benign (1).png" -> "benign (1)_mask.png"
        mask_name = img_path.stem + '_mask.png'
        mask_path = category_dir / mask_name
        
        if not mask_path.exists():
            print(f"  Warning: Mask not found for {img_path.name}, skipping...")
            continue
        
        # ç”Ÿæˆå”¯ä¸€ ID
        sample_id = f"BUSI_{category}_{sample_idx:06d}"
        
        samples.append({
            'id': sample_id,
            'img_path': str(img_path),
            'mask_path': str(mask_path),
            'label_name': category,
        })
        
        sample_idx += 1
    
    return samples


def split_samples(
    samples: List[Dict],
    val_ratio: float = 0.2,
    test_ratio: float = 0.0,
    seed: int = 42,
) -> Tuple[List[Dict], List[Dict], Optional[List[Dict]]]:
    """
    å°†æ ·æœ¬åˆ’åˆ†ä¸º train / val / test
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        (train_samples, val_samples, test_samples)
        test_samples å¯èƒ½ä¸º Noneï¼ˆå½“ test_ratio=0ï¼‰
    """
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # æ‰“ä¹±æ ·æœ¬
    samples_shuffled = samples.copy()
    random.shuffle(samples_shuffled)
    
    n_total = len(samples_shuffled)
    
    # è®¡ç®—å„ä¸ª split çš„æ ·æœ¬æ•°
    if test_ratio > 0:
        n_test = int(n_total * test_ratio)
        n_val = int((n_total - n_test) * val_ratio)
        n_train = n_total - n_test - n_val
        
        test_samples = samples_shuffled[:n_test]
        val_samples = samples_shuffled[n_test:n_test + n_val]
        train_samples = samples_shuffled[n_test + n_val:]
    else:
        n_val = int(n_total * val_ratio)
        n_train = n_total - n_val
        
        val_samples = samples_shuffled[:n_val]
        train_samples = samples_shuffled[n_val:]
        test_samples = None
    
    print(f"\nDataset split:")
    print(f"  Train: {len(train_samples)} samples")
    print(f"  Val: {len(val_samples)} samples")
    if test_samples is not None:
        print(f"  Test: {len(test_samples)} samples")
    
    return train_samples, val_samples, test_samples


def load_and_convert_image(img_path: str) -> np.ndarray:
    """
    åŠ è½½å¹¶è½¬æ¢å›¾åƒä¸ºç°åº¦ float32 æ•°ç»„
    
    Args:
        img_path: å›¾åƒè·¯å¾„
    
    Returns:
        np.ndarray, shape (H, W), dtype float32
    """
    img = Image.open(img_path).convert('L')  # è½¬ä¸ºç°åº¦
    img_np = np.array(img, dtype=np.float32)
    return img_np


def load_and_convert_mask(mask_path: str) -> np.ndarray:
    """
    åŠ è½½å¹¶è½¬æ¢ mask ä¸ºäºŒå€¼æ•°ç»„
    
    Args:
        mask_path: mask è·¯å¾„
    
    Returns:
        np.ndarray, shape (H, W), dtype float32, å€¼ä¸º 0 æˆ– 1
    """
    mask = Image.open(mask_path).convert('L')  # è½¬ä¸ºç°åº¦
    mask_np = np.array(mask, dtype=np.float32)
    
    # äºŒå€¼åŒ–: >0 ä¸º 1ï¼Œå¦åˆ™ä¸º 0
    mask_np = (mask_np > 0).astype(np.float32)
    
    return mask_np


def preprocess_and_save(
    samples: List[Dict],
    output_root: str,
    val_ratio: float = 0.2,
    test_ratio: float = 0.0,
    seed: int = 42,
) -> None:
    """
    é¢„å¤„ç†æ ·æœ¬å¹¶ä¿å­˜ä¸º npy + metadata JSON
    
    ğŸ”¥ ä¿®æ”¹ï¼šæ‰€æœ‰æ ·æœ¬éƒ½æœ‰ maskï¼ˆnormal å·²è¢«æ’é™¤ï¼‰
    
    Args:
        samples: æ”¶é›†åˆ°çš„æ ·æœ¬åˆ—è¡¨
        output_root: è¾“å‡ºæ ¹ç›®å½•
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    """
    output_root = Path(output_root)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    images_dir = output_root / 'images'
    masks_dir = output_root / 'masks'
    images_dir.mkdir(parents=True, exist_ok=True)
    masks_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nOutput directory: {output_root}")
    print(f"  Images: {images_dir}")
    print(f"  Masks: {masks_dir}")
    
    # åˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_samples(
        samples, val_ratio, test_ratio, seed
    )
    
    # å¤„ç†æ‰€æœ‰æ ·æœ¬
    all_splits = {
        'train': train_samples,
        'val': val_samples,
    }
    if test_samples is not None:
        all_splits['test'] = test_samples
    
    metadata_all = {}
    
    for split_name, samples_in_split in all_splits.items():
        print(f"\nProcessing {split_name} split ({len(samples_in_split)} samples)...")
        
        metadata = []
        
        for sample in tqdm(samples_in_split, desc=f"  Converting {split_name}"):
            sample_id = sample['id']
            
            # åŠ è½½å¹¶ä¿å­˜å›¾åƒ
            img_np = load_and_convert_image(sample['img_path'])
            img_save_path = images_dir / f"{sample_id}.npy"
            np.save(img_save_path, img_np)
            
            # ğŸ”¥ æ‰€æœ‰æ ·æœ¬éƒ½æœ‰ maskï¼ˆå› ä¸º normal å·²è¢«æ’é™¤ï¼‰
            mask_np = load_and_convert_mask(sample['mask_path'])
            mask_save_path = masks_dir / f"{sample_id}_mask.npy"
            np.save(mask_save_path, mask_np)
            
            # æ„é€  metadata æ¡ç›®
            meta_entry = {
                'id': sample_id,
                'image_path': f"images/{sample_id}.npy",
                'mask_path': f"masks/{sample_id}_mask.npy",
                'label': LABEL_MAP[sample['label_name']],
                'domain': 'BUSI',
            }
            
            metadata.append(meta_entry)
        
        metadata_all[split_name] = metadata
    
    # ä¿å­˜ metadata JSON
    for split_name, metadata in metadata_all.items():
        json_path = output_root / f'{split_name}_meta.json'
        with open(json_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"\nSaved {split_name} metadata: {json_path}")
        print(f"  Samples: {len(metadata)}")
    
    print(f"\nâœ… Preprocessing complete!")
    print(f"ğŸ”¥ Only benign and malignant samples included")


def quick_test(output_root: str, image_size: Tuple[int, int] = (256, 256)) -> None:
    """
    å¿«é€Ÿæµ‹è¯•ï¼šéªŒè¯é¢„å¤„ç†ç»“æœå¯ä»¥è¢« DataLoader æ­£ç¡®åŠ è½½
    
    Args:
        output_root: é¢„å¤„ç†è¾“å‡ºç›®å½•
        image_size: æµ‹è¯•æ—¶çš„å›¾åƒå°ºå¯¸
    """
    print("\n" + "=" * 60)
    print("Quick Test: DataLoader Compatibility")
    print("=" * 60)
    
    output_root = Path(output_root)
    train_meta_path = output_root / 'train_meta.json'
    
    if not train_meta_path.exists():
        print(f"Error: Metadata file not found: {train_meta_path}")
        return
    
    # å°è¯•å¯¼å…¥é€šç”¨ DataLoader
    try:
        # å‡è®¾ dataset æ¨¡å—åœ¨ Python path ä¸­
        import sys
        
        # å°è¯•æ·»åŠ å¯èƒ½çš„è·¯å¾„
        possible_paths = [
            Path.cwd() / 'dataset',
            Path.cwd().parent / 'dataset',
        ]
        for p in possible_paths:
            if p.exists():
                sys.path.insert(0, str(p.parent))
                break
        
        from dataset.meta.UltrasoundSample import load_ultrasound_metadata
        from dataset.dataset.ultrasound_segmentation import UltrasoundSegmentationDataset
        from torch.utils.data import DataLoader
        import torch
        
        print("\nâœ… Successfully imported DataLoader components")
        
        # åŠ è½½ metadata
        samples = load_ultrasound_metadata(str(train_meta_path))
        print(f"\nâœ… Loaded {len(samples)} samples from metadata")
        
        # åˆ›å»º Dataset
        dataset = UltrasoundSegmentationDataset(
            samples=samples,
            image_size=image_size,
            augment=None,
            normalize=True,
            return_label=True,
            return_domain=True,
        )
        
        # åˆ›å»º DataLoader
        loader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,
        )
        
        # è·å–ä¸€ä¸ª batch
        batch = next(iter(loader))
        
        print(f"\nâœ… Successfully loaded one batch:")
        print(f"  images: {batch['image'].shape}")
        print(f"  masks: {batch['mask'].shape}")
        print(f"  labels: {batch['label']}")
        print(f"  domains: {batch['domain']}")
        
        # éªŒè¯æ•°æ®èŒƒå›´
        print(f"\nData statistics:")
        print(f"  Image range: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]")
        print(f"  Image mean: {batch['image'].mean():.3f}")
        
        unique_mask_values = torch.unique(batch['mask'])
        print(f"  Mask unique values: {unique_mask_values.tolist()}")
        
        # ğŸ”¥ éªŒè¯æ²¡æœ‰ normal æ ‡ç­¾
        unique_labels = torch.unique(batch['label'])
        print(f"  Label unique values: {unique_labels.tolist()}")
        print(f"  ğŸ”¥ Expected labels: 1 (benign), 2 (malignant)")
        
        print("\nâœ… DataLoader compatibility test passed!")
        
    except ImportError as e:
        print(f"\nâš ï¸  Could not import DataLoader components: {e}")
        print("Skipping DataLoader test. Please ensure dataset modules are in Python path.")
        
        # ç®€å•éªŒè¯ï¼šç›´æ¥è¯»å–æ–‡ä»¶
        print("\nPerforming basic file validation instead...")
        
        with open(train_meta_path, 'r') as f:
            metadata = json.load(f)
        
        print(f"\nMetadata validation:")
        print(f"  Total entries: {len(metadata)}")
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        label_counts = {}
        for item in metadata:
            label = item['label']
            label_counts[label] = label_counts.get(label, 0) + 1
        
        print(f"\nLabel distribution:")
        for label, count in sorted(label_counts.items()):
            label_name = 'benign' if label == 1 else 'malignant'
            print(f"  {label_name} (label={label}): {count} samples")
        
        # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬
        for i, item in enumerate(metadata[:3]):
            print(f"\nSample {i+1}:")
            print(f"  ID: {item['id']}")
            
            img_path = output_root / item['image_path']
            print(f"  Image exists: {img_path.exists()}")
            
            mask_path = output_root / item['mask_path']
            print(f"  Mask exists: {mask_path.exists()}")
            
            print(f"  Label: {item['label']}")
            print(f"  Domain: {item['domain']}")
        
        print("\nâœ… Basic file validation passed!")
        print("ğŸ”¥ All samples have masks (normal excluded)")
    
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(
        description="Preprocess BUSI dataset (exclude normal samples)"
    )
    
    parser.add_argument(
        '--busi_root',
        type=str,
        required=True,
        help='Path to original BUSI dataset root directory'
    )
    
    parser.add_argument(
        '--output_root',
        type=str,
        required=True,
        help='Path to output directory for preprocessed data'
    )
    
    parser.add_argument(
        '--val_ratio',
        type=float,
        default=0.2,
        help='Validation set ratio (default: 0.2)'
    )
    
    parser.add_argument(
        '--test_ratio',
        type=float,
        default=0.0,
        help='Test set ratio (default: 0.0, no test set)'
    )
    
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for dataset splitting (default: 42)'
    )
    
    parser.add_argument(
        '--overwrite',
        action='store_true',
        help='Allow overwriting existing output directory'
    )
    
    parser.add_argument(
        '--skip_test',
        action='store_true',
        help='Skip the quick DataLoader test after preprocessing'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_root = Path(args.output_root)
    if output_root.exists() and not args.overwrite:
        response = input(
            f"Output directory already exists: {output_root}\n"
            f"Do you want to overwrite it? (y/n): "
        )
        if response.lower() != 'y':
            print("Aborted.")
            return
        else:
            shutil.rmtree(output_root)
    
    # å¼€å§‹é¢„å¤„ç†
    print("=" * 60)
    print("BUSI Dataset Preprocessing")
    print("ğŸ”¥ Normal samples will be excluded")
    print("=" * 60)
    print(f"\nInput: {args.busi_root}")
    print(f"Output: {args.output_root}")
    print(f"Val ratio: {args.val_ratio}")
    print(f"Test ratio: {args.test_ratio}")
    print(f"Random seed: {args.seed}")
    
    # æ”¶é›†æ ·æœ¬ï¼ˆæ’é™¤ normalï¼‰
    samples = collect_busi_samples(args.busi_root)
    
    if len(samples) == 0:
        print("\nError: No samples collected. Please check the input directory structure.")
        return
    
    # é¢„å¤„ç†å¹¶ä¿å­˜
    preprocess_and_save(
        samples=samples,
        output_root=args.output_root,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        seed=args.seed,
    )
    
    # å¿«é€Ÿæµ‹è¯•
    if not args.skip_test:
        quick_test(args.output_root)
    
    print("\n" + "=" * 60)
    print("All done! ğŸ‰")
    print("ğŸ”¥ Dataset contains only benign and malignant samples")
    print("=" * 60)


if __name__ == '__main__':
    main()